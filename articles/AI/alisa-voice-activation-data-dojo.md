# Как работает распознавание звука в Алисе — доклад Дмитрия Солодухи (Data Dojo)

**Type:** article

**Summary:** Доклад Дмитрия Солодухи (Data Dojo, Яндекс) о том, как устроена локальная модель голосовой активации Алисы, её архитектура, фичи и работа в шумных условиях.

**Tags:** #voice-activation #speech-recognition #voice-assistants #yandex

## Links

- https://vkvideo.ru/video-220652761_456240008

## Original Message

Как работает распознавание звука в Алисе

На прошлой неделе в Москве прошла большая встреча ML-комьюнити Data Dojo от Яндекса. Такие ивенты ценим прежде всего за интересные технические ML-ные доклады. В этот раз слушали их на прямой трансляции.

Больше всего зашло выступление от руководителя команды голосовой активации Дмитрия Солодуха под названием "Кухня, гости, музыка: как мы научили колонку реагировать в реальном хаосе".

Наверное, многим уже верхеуровнево известно, как активируется Алиса: внутри нее сидит небольшая локальная модель голосовой активации, которая работает на устройстве. Она цепляет из всего входящего голосового потока заданные наборы фраз с целью понять, когда пользователь начинает обращаться к ассистенту. Как только слово “Алиса” распознано локально – команда отправляется на сервер, где она обрабатывается. 

Но если в целом принцип работы системы понятен, то многие детали могут быть не так очевидны. Например, вы задумывались, как такая крохотная модель распознает звук, если у вас одновременно играет музыка, кричат дети, разговаривают гости? Или как Алиса понимает, что вы обращаетесь к ней, если говорите просто "громче", а не "Алиса, громче"? Или какая там используется архитектура и фичи? 

Обо всем этом как раз рассказал Дмитрий в своем [докладе](https://vkvideo.ru/video-220652761_456240008). Будет полезно тем, кто работает со сложными мультимодальными системами,  и спецам, кто так или иначе занимается голосом и классификацией. Внутри много конкретных ML-деталей, о которых редко рассказывают публично.

---
*Saved from Telegram on 2026-01-08*
